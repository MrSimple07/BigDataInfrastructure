{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVwP89jmqdZ34e2xi5UsJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/Big_Data_Technologies/blob/main/BigDataTechnologies_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Consepts of Big Data\n",
        "\n",
        "### 1. What is Big Data? History of Big Data, V-concepts\n",
        "\n",
        "Big Data is the large data information that grow at ever- increasing rates.\n",
        "\n",
        "Big data is characterized by 3 V's - Volume, Velocity, Variety\n",
        "\n",
        "- Volume - The quantity of generated and stored data.\n",
        "- Velocity - The speed at which new data is generated and moved\n",
        "- Variety  - The different types of data, both structured and unstructured.\n",
        "\n",
        "Over time 2 V's also added:\n",
        "- Veracity - The quality of data\n",
        "- Value - The usefullness of data\n",
        "\n",
        "And also:\n",
        "- Visualization\n",
        "- Vulnerability: The security of data\n",
        "- Volatility - The lifespan of data. How long data is valid and useful\n",
        "- Variability - The inconsistency of data flows.\n",
        "\n",
        "1990th years \"Big Data\" term gains popularity. The first statistical data analysis used in 1663 year by John Graunt.\n",
        "\n",
        "2000s - Google published its paper on the MapReduce algorithm. And Apache Hadoop introduced and it was revolutionary."
      ],
      "metadata": {
        "id": "QLxze36700nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Streaming solutions for Big Data\n",
        "\n",
        "Streaming solution means that analyzing data in the real time. They are applied for different applications:\n",
        "\n",
        "- Fraud Detection: Real - Time monitoring of transactions to detect fraud actions\n",
        "- IoT Data Processing\n",
        "- Recommendation Engines: Offering real-time personalized recommendations.\n",
        "\n",
        "Solutions:\n",
        "- 1. Apache Kafka\n",
        "- 2. Apache Flink\n",
        "- 3. Apache Storm\n",
        "- 4. Google Cloud Dataflow\n",
        "- 5. Amazon Kinesis\n",
        "- 6. Azure Stream Analytics by Microsoft Azure"
      ],
      "metadata": {
        "id": "OIyiLWo5XzWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Storages and resource managers - supplementary tools\n",
        "\n",
        "Storage Solutions:\n",
        "1. Hadoop Distributed File System (HDFS)\n",
        "2. Amazon S3 (Simple Storage Service)\n",
        "3. Google Cloud Storage\n",
        "4. Azure Blob Storage\n",
        "5. Apache Cassandra\n",
        "6. Apache HBase\n",
        "7. ElasticSearch\n",
        "\n",
        "\n",
        "Resource Managers:\n",
        "1. Apache YARN (Yet another Resource Negotiator)\n",
        "2. Apache Mesos\n",
        "3. Kubernetes\n",
        "4. Apache ZooKeper"
      ],
      "metadata": {
        "id": "RREnwOjrbEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is HDFS? HDFS Architecture. What is a chunk? How are chunks being processed into records? How to customize the way chunks are being processed?\n",
        "\n",
        "\n",
        "- HDFS - Hadoop Distributed File System.\n",
        "Distributed storage system designed to store large datasets reliably and to stream those data sets at high bandwidth to user applications.\n",
        "\n",
        "- HDFS is the main component of the Apache Hadoop. It follows a master- slave architecture:\n",
        "\n",
        "1. NameNode - The master server that manages the file system namespace.\n",
        "2. Datanodes -  DataNodes are the worker nodes that store and retrieve data blocks when they are instructed to do so by the Namenode.\n",
        "3. Secondary Namenode - It periodically merges the namespace image with the edit logs to prevent the edit logs from growing too large. It helps to reduce the Namenode’s startup time by keeping the namespace image current.\n",
        "4. Clients - They communicate with the Namenode to get the metadata and then interact directly with DataNodes to read/write the actual data.\n",
        "\n",
        "Chunks - large files, divided into smaller fixed- size blocks, typically called chunks or blocks. The default is 128 mb, can be configured."
      ],
      "metadata": {
        "id": "sTO2zyC5csQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Replica in HDFS. Safe mode. NameNode. DataNode.\n",
        "\n",
        "\n",
        "- Safe mode is the read only mode of the NameNode\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ud1d2UbcvNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. CAP theorem. Communication Protocols. Data consistency.\n",
        "\n",
        "- Consistency\n",
        "- Availability\n",
        "- Partition Tolerance\n"
      ],
      "metadata": {
        "id": "eUe4qFBM1U2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Data Formats: txt, csv, binary, json, parquet. What's better?\n",
        "\n",
        "TXT - Plain text\n",
        "\n",
        "CSV - Comma Separated Values. Great for tabular data\n",
        "\n",
        "JSON - JavaScript Object Notation. Ideal for the Web applications and APIs needing flexible and human- readable data.\n",
        "\n",
        "Parquet - Columnar storage file format optimized for use with Big Data Processing frameworks. Efficient for Big Data applications requiring efficient storage and fast analytical queries.\n"
      ],
      "metadata": {
        "id": "yGuDx_pZ1dbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Spark\n",
        "\n",
        "### 1. What is Apache Spark? Which components does its architecture consist of?\n",
        "\n",
        "Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
        "\n",
        "Components of Apache Spark Architectures:\n",
        "- Driver Program\n",
        "- SparkContext\n",
        "- Cluster Manager\n",
        "- Executors\n",
        "- Tasks\n",
        "- DAG Scheduler"
      ],
      "metadata": {
        "id": "YCC9RNNJ1hb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is RDD? Which principal components does it consist of? What is a transformation? What is an action?\n",
        "\n",
        "Resilient Distributed Dataset (RDD) - is the fundamental dataset for the Spark. It is an immutable distributed collection of objects that can be processed in parallel across a cluster.\n",
        "\n",
        "Principal Components of RDD:\n",
        "1. Partitions\n",
        "2. Dependencies\n",
        "3. Partitions Iterator\n",
        "4. Partitioner\n",
        "5. Computer Function\n",
        "\n",
        "Transformations in Spark are the lazy operations to define a new RDDs. They don't update an RDD, they just retun another RDD.\n",
        "\n",
        "RDD actions - operations that trigger computation and return RDD values."
      ],
      "metadata": {
        "id": "NWiB1zUx7Ltu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is RDD lineage? How is it formed? How can different RDD lineages be combined?\n",
        "\n",
        "RDD lineage is a fundamental concept in Apache Spark that captures the series of transformations leading to an RDD's creation."
      ],
      "metadata": {
        "id": "nrdooReP7PSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is a partition in RDD? How does a partition in RDD relate to a partition in HDFS?"
      ],
      "metadata": {
        "id": "mCWJnSX57Rcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. What is broadcast and a broadcasting variable in Apache Spark? What goals does it serve? Give an example of its usage.\n",
        "\n",
        "## 6. What is caching and persistence in Apache Spark? What is the difference between caching and persistence in Apache Spark? Which modes of persistence exist? What is spillover?\n",
        "\n",
        "\n",
        "## 7. What is shuffling? Explain how it works."
      ],
      "metadata": {
        "id": "kLfeczZG1wkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL\n",
        "\n",
        "### 1. What is SparkSQL? What is SparkSession? What is DataFrame? What is Column? Name differences between RDD and DataFrame. What is Tungsten?\n",
        "\n",
        "- SparkSQL is the module of Apache Spark for structured data processing.\n",
        "\n",
        "With SparkSQL we can work with SQL, and DataFrame API.\n",
        "\n",
        "Spark Session - the entry point to programming Spark with the Dataset and DataFrame API."
      ],
      "metadata": {
        "id": "LBq67JkE12LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName('ExampleApp')\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "1ZHSYu2F7Lrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DataFrame in SparkSQL is a distributed collection of data organized into named columns = data frame in R/Python.\n",
        "\n",
        "Data Frame can be created by Hive, external databases, or existing RDDs."
      ],
      "metadata": {
        "id": "w4zYbRUX7W6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "XcmEj7eG8AhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Difference between RDD and dataframe in Spark\n",
        "\n",
        "1. DataFrame is faster, higher- level, schema- based, and optimized operations.\n",
        "\n",
        "2. RDD is low- level, unoptimized, and schema-less"
      ],
      "metadata": {
        "id": "zkb6T2UY8BGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tungsten - A project to enhance Spark's performance through optimizations in memory management, binary processing and code generation"
      ],
      "metadata": {
        "id": "aCph5TQS8gMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL5I2oEM8yGy",
        "outputId": "10847550-1a30-487c-c26c-86f575acaec9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0b0f5173225facf9f88b49aad33000bc9ad54f034bf34603ce6becfe2226e005\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TungstenExample\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a simple DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29), (\"David\", 42), (\"Eve\", 36)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Perform some transformations\n",
        "df_filtered = df.filter(col(\"Age\") > 30)\n",
        "df_avg_age = df_filtered.agg(avg(\"Age\").alias(\"Average Age\"))\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "print(\"Filtered DataFrame with Average Age:\")\n",
        "df_avg_age.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbQkI_s780h4",
        "outputId": "a4dc1bd6-b2c7-4425-870a-001c13f59d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|      Bob| 45|\n",
            "|Catherine| 29|\n",
            "|    David| 42|\n",
            "|      Eve| 36|\n",
            "+---------+---+\n",
            "\n",
            "Filtered DataFrame with Average Age:\n",
            "+-----------+\n",
            "|Average Age|\n",
            "+-----------+\n",
            "|      39.25|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. UDF in Spark SQL. How to write an UDF (give a simple example)? What pitfalls should you be aware about when writing a custom udf?\n",
        "\n"
      ],
      "metadata": {
        "id": "HVmJ2PZT6fQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is a schema of DataFrame? How to get a schema of DataFrame? What is groupby and how does it work?\n",
        "\n"
      ],
      "metadata": {
        "id": "rZNyTWZ86hrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Supported join operations in Spark SQL. Broadcast joins. Basic principles of functioning. Which types of broadcast-based joins exist and what are the differences in their functioning? Conditions of when one can apply these types of joins.\n",
        "\n"
      ],
      "metadata": {
        "id": "6fKvyKA06khS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Sort Merge Join. What is it for? Basic principles of functioning. Conditions of when one can apply these types of joins. Advantages and disadvantages in comparison with other joins."
      ],
      "metadata": {
        "id": "T6nPLQBC6nAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Bucketing: what it is, its advantages and basic principles of functioning."
      ],
      "metadata": {
        "id": "nu56XFDb6o68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClickHouse\n",
        "\n",
        "## 1.What is ClickHouse? Which components does its architecture consist of?\n",
        "\n",
        "## 2. What is Materialized View and what is its purpose? How does Materialized Views work? Does it make sense to create a materialized view over a distributed table? If the answer is positive, why?\n",
        "\n",
        "## 3. Describe MergeTree engine work principles. How does ClickHouse store data in the filesystem? How does parts merging works from in terms of files?\n",
        "\n",
        "## 4. What is sharding, why is it useful and how is it implemented in ClickHouse? How can one distribute data into shards using ClickHouse?\n",
        "\n",
        "## 5. How does AggregatingMergeTree work? Are there any differences between aggregation functions in AggregatingMergeTree and simple queries?\n",
        "\n",
        "## 6. Why is it recommended inserting data into ClickHouse by large batches? What can one do if data arrives in short portions frequently? Name and explain existing mechanisms in ClickHouse to deal with the problem.\n",
        "\n",
        "## 7. Describe sparse index work principles. How is it implemented in ClickHouse? How can we set up indexes in ClickHouse?"
      ],
      "metadata": {
        "id": "g01UoBuc2WbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kubernetes\n",
        "\n",
        "## 1. What is Kubernetes? Its architecture, main components.\n",
        "\n",
        "## 2. What is Pod? Difference between pod and container.\n",
        "\n",
        "## 3. What is Service? How does it work? Service types: nodeport, clusterip, headless service.\n",
        "\n",
        "## 4. Container resources: how is it implemented? CPU, RAM, Storage. QoS classes.\n",
        "\n",
        "## 5. PV & PVC: what is it and how does it work? PV types. Volume provisioner.\n",
        "\n",
        "## 6. StatefulSet. Difference from Deployment/Pod.\n",
        "\n",
        "## 7. Label selector. What is it? How is it used?"
      ],
      "metadata": {
        "id": "1D9jUmwq2niK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7iHEtxiWb9q"
      },
      "outputs": [],
      "source": []
    }
  ]
}