{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/qCCxcWdb52Qky7wqmfVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/Big_Data_Technologies/blob/main/BigDataTechnologies_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Consepts of Big Data\n",
        "\n",
        "### 1. What is Big Data? History of Big Data, V-concepts\n",
        "\n",
        "Big Data is the large data information that grow at ever- increasing rates.\n",
        "\n",
        "Big data is characterized by 3 V's - Volume, Velocity, Variety\n",
        "\n",
        "- Volume - The quantity of generated and stored data.\n",
        "- Velocity - The speed at which new data is generated and moved\n",
        "- Variety  - The different types of data, both structured and unstructured.\n",
        "\n",
        "Over time 2 V's also added:\n",
        "- Veracity - The quality of data\n",
        "- Value - The usefullness of data\n",
        "\n",
        "And also:\n",
        "- Visualization\n",
        "- Vulnerability: The security of data\n",
        "- Volatility - The lifespan of data. How long data is valid and useful\n",
        "- Variability - The inconsistency of data flows.\n",
        "\n",
        "1990th years \"Big Data\" term gains popularity. The first statistical data analysis used in 1663 year by John Graunt.\n",
        "\n",
        "2000s - Google published its paper on the MapReduce algorithm. And Apache Hadoop introduced and it was revolutionary."
      ],
      "metadata": {
        "id": "QLxze36700nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Streaming solutions for Big Data\n",
        "\n",
        "Streaming solution means that analyzing data in the real time. They are applied for different applications:\n",
        "\n",
        "- Fraud Detection: Real - Time monitoring of transactions to detect fraud actions\n",
        "- IoT Data Processing\n",
        "- Recommendation Engines: Offering real-time personalized recommendations.\n",
        "\n",
        "Solutions:\n",
        "- 1. Apache Kafka\n",
        "- 2. Apache Flink\n",
        "- 3. Apache Storm\n",
        "- 4. Google Cloud Dataflow\n",
        "- 5. Amazon Kinesis\n",
        "- 6. Azure Stream Analytics by Microsoft Azure"
      ],
      "metadata": {
        "id": "OIyiLWo5XzWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Storages and resource managers - supplementary tools\n",
        "\n",
        "Storage Solutions:\n",
        "1. Hadoop Distributed File System (HDFS)\n",
        "2. Amazon S3 (Simple Storage Service)\n",
        "3. Google Cloud Storage\n",
        "4. Azure Blob Storage\n",
        "5. Apache Cassandra\n",
        "6. Apache HBase\n",
        "7. ElasticSearch\n",
        "\n",
        "\n",
        "Resource Managers:\n",
        "1. Apache YARN (Yet another Resource Negotiator)\n",
        "2. Apache Mesos\n",
        "3. Kubernetes\n",
        "4. Apache ZooKeper"
      ],
      "metadata": {
        "id": "RREnwOjrbEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is HDFS? HDFS Architecture. What is a chunk? How are chunks being processed into records? How to customize the way chunks are being processed?\n",
        "\n",
        "\n",
        "- HDFS - Hadoop Distributed File System.\n",
        "Distributed storage system designed to store large datasets reliably and to stream those data sets at high bandwidth to user applications.\n",
        "\n",
        "- HDFS is the main component of the Apache Hadoop. It follows a master- slave architecture:\n",
        "\n",
        "1. NameNode - The master server that manages the file system namespace.\n",
        "2. Datanodes -  DataNodes are the worker nodes that store and retrieve data blocks when they are instructed to do so by the Namenode.\n",
        "3. Secondary Namenode - It periodically merges the namespace image with the edit logs to prevent the edit logs from growing too large. It helps to reduce the Namenode’s startup time by keeping the namespace image current.\n",
        "4. Clients - They communicate with the Namenode to get the metadata and then interact directly with DataNodes to read/write the actual data.\n",
        "\n",
        "Chunks - large files, divided into smaller fixed- size blocks, typically called chunks or blocks. The default is 128 mb, can be configured."
      ],
      "metadata": {
        "id": "sTO2zyC5csQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Replica in HDFS. Safe mode. NameNode. DataNode.\n",
        "\n",
        "\n",
        "- Safe mode is the read only mode of the NameNode\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ud1d2UbcvNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. CAP theorem. Communication Protocols. Data consistency.\n",
        "\n",
        "- Consistency\n",
        "- Availability\n",
        "- Partition Tolerance\n"
      ],
      "metadata": {
        "id": "eUe4qFBM1U2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Data Formats: txt, csv, binary, json, parquet. What's better?\n",
        "\n",
        "TXT - Plain text\n",
        "\n",
        "CSV - Comma Separated Values. Great for tabular data\n",
        "\n",
        "JSON - JavaScript Object Notation. Ideal for the Web applications and APIs needing flexible and human- readable data.\n",
        "\n",
        "Parquet - Columnar storage file format optimized for use with Big Data Processing frameworks. Efficient for Big Data applications requiring efficient storage and fast analytical queries.\n"
      ],
      "metadata": {
        "id": "yGuDx_pZ1dbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Spark\n",
        "\n",
        "### 1. What is Apache Spark? Which components does its architecture consist of?\n",
        "\n",
        "Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
        "\n",
        "Components of Apache Spark Architectures:\n",
        "- Driver Program\n",
        "- SparkContext\n",
        "- Cluster Manager\n",
        "- Executors\n",
        "- Tasks\n",
        "- DAG Scheduler"
      ],
      "metadata": {
        "id": "YCC9RNNJ1hb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is RDD? Which principal components does it consist of? What is a transformation? What is an action?\n",
        "\n",
        "Resilient Distributed Dataset (RDD) - is the fundamental dataset for the Spark. It is an immutable distributed collection of objects that can be processed in parallel across a cluster.\n",
        "\n",
        "Principal Components of RDD:\n",
        "1. Partitions\n",
        "2. Dependencies\n",
        "3. Partitions Iterator\n",
        "4. Partitioner\n",
        "5. Computer Function\n",
        "\n",
        "Transformations in Spark are the lazy operations to define a new RDDs. They don't update an RDD, they just retun another RDD.\n",
        "\n",
        "RDD actions - operations that trigger computation and return RDD values."
      ],
      "metadata": {
        "id": "NWiB1zUx7Ltu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is RDD lineage? How is it formed? How can different RDD lineages be combined?\n",
        "\n",
        "RDD lineage is a fundamental concept in Apache Spark that captures the series of transformations leading to an RDD's creation."
      ],
      "metadata": {
        "id": "nrdooReP7PSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is a partition in RDD? How does a partition in RDD relate to a partition in HDFS?"
      ],
      "metadata": {
        "id": "mCWJnSX57Rcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. What is broadcast and a broadcasting variable in Apache Spark? What goals does it serve? Give an example of its usage.\n",
        "\n",
        "## 6. What is caching and persistence in Apache Spark? What is the difference between caching and persistence in Apache Spark? Which modes of persistence exist? What is spillover?\n",
        "\n",
        "\n",
        "## 7. What is shuffling? Explain how it works."
      ],
      "metadata": {
        "id": "kLfeczZG1wkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL\n",
        "\n",
        "### 1. What is SparkSQL? What is SparkSession? What is DataFrame? What is Column? Name differences between RDD and DataFrame. What is Tungsten?\n",
        "\n",
        "- SparkSQL is the module of Apache Spark for structured data processing.\n",
        "\n",
        "With SparkSQL we can work with SQL, and DataFrame API.\n",
        "\n",
        "Spark Session - the entry point to programming Spark with the Dataset and DataFrame API."
      ],
      "metadata": {
        "id": "LBq67JkE12LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName('ExampleApp')\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "1ZHSYu2F7Lrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DataFrame in SparkSQL is a distributed collection of data organized into named columns = data frame in R/Python.\n",
        "\n",
        "Data Frame can be created by Hive, external databases, or existing RDDs."
      ],
      "metadata": {
        "id": "w4zYbRUX7W6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "XcmEj7eG8AhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Difference between RDD and dataframe in Spark\n",
        "\n",
        "1. DataFrame is faster, higher- level, schema- based, and optimized operations.\n",
        "\n",
        "2. RDD is low- level, unoptimized, and schema-less"
      ],
      "metadata": {
        "id": "zkb6T2UY8BGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tungsten - A project to enhance Spark's performance through optimizations in memory management, binary processing and code generation"
      ],
      "metadata": {
        "id": "aCph5TQS8gMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mL5I2oEM8yGy",
        "outputId": "10847550-1a30-487c-c26c-86f575acaec9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0b0f5173225facf9f88b49aad33000bc9ad54f034bf34603ce6becfe2226e005\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TungstenExample\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a simple DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29), (\"David\", 42), (\"Eve\", 36)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Perform some transformations\n",
        "df_filtered = df.filter(col(\"Age\") > 30)\n",
        "df_avg_age = df_filtered.agg(avg(\"Age\").alias(\"Average Age\"))\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "print(\"Filtered DataFrame with Average Age:\")\n",
        "df_avg_age.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MbQkI_s780h4",
        "outputId": "a4dc1bd6-b2c7-4425-870a-001c13f59d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|      Bob| 45|\n",
            "|Catherine| 29|\n",
            "|    David| 42|\n",
            "|      Eve| 36|\n",
            "+---------+---+\n",
            "\n",
            "Filtered DataFrame with Average Age:\n",
            "+-----------+\n",
            "|Average Age|\n",
            "+-----------+\n",
            "|      39.25|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. UDF in Spark SQL. How to write an UDF (give a simple example)? What pitfalls should you be aware about when writing a custom udf?\n",
        "\n",
        "\n",
        "- UDF: User- defined Function\n",
        "\n",
        "UDF is a custom function that we can create and register to extend the capabilities of SparkSQL.\n",
        "\n",
        "Allow to apply custom logic to the data within DataFrames and SQL queries\n"
      ],
      "metadata": {
        "id": "HVmJ2PZT6fQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating sparkSession and dataframe\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('UDFExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)"
      ],
      "metadata": {
        "id": "9m0ERFCb-Jpl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def age_group(age):\n",
        "    if age < 35:\n",
        "        return \"Young\"\n",
        "    else:\n",
        "        return \"Adult\"\n",
        "\n",
        "# Register the function as a UDF\n",
        "age_group_udf = udf(age_group, StringType())\n",
        "\n",
        "\n",
        "df_with_age_group = df.withColumn(\"AgeGroup\", age_group_udf(df[\"Age\"]))\n",
        "df_with_age_group.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1cmSg6E_FDK",
        "outputId": "f54226e4-eaa1-4845-a6fb-c5e3ef0a9ef6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+--------+\n",
            "|     Name|Age|AgeGroup|\n",
            "+---------+---+--------+\n",
            "|    Alice| 34|   Young|\n",
            "|      Bob| 45|   Adult|\n",
            "|Catherine| 29|   Young|\n",
            "+---------+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pitfalls to be aware of when writing a custom UDF:\n",
        "\n",
        "- Performance overhead: it is slow. And the data should be serialized and deserialized between JVM and Python, and it can be expensive\n",
        "\n",
        "- Optimization: Lack of Optimization, which means they can be less efficient compared to built-in functions.\n",
        "\n",
        "- Debugging: Debugging is challenging due to their custom nature and execution in a distributed environment.\n",
        "\n",
        "- Reusability issues: We cannot reuse it and the custom function is just for the session that we registered."
      ],
      "metadata": {
        "id": "DCDCFZ3m_6HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is a schema of DataFrame? How to get a schema of DataFrame? What is groupby and how does it work?\n",
        "\n",
        "- A schema is the structure of the data, including the columna names, data types, and nullable properties.\n",
        "\n",
        "\"printSchema\" to get the schema of DataFrame."
      ],
      "metadata": {
        "id": "rZNyTWZ86hrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('schemaExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", \"HR\", 34), (\"Bob\", \"IT\", 45), (\"Catherine\", \"HR\", 29), (\"David\", \"IT\", 42), (\"Eve\", \"Finance\", 36)]\n",
        "columns = [\"Name\", \"Department\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "df_grouped = df.groupby(\"Department\").agg(avg('Age').alias('Average Age'))\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZH-oy_rBn0Z",
        "outputId": "40fcfaa0-fa5c-4d26-cf87-474abb2898ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|Average Age|\n",
            "+----------+-----------+\n",
            "|        HR|       31.5|\n",
            "|        IT|       43.5|\n",
            "|   Finance|       36.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz_5Bo75CUtM",
        "outputId": "a5ddda1a-28e3-4757-eab6-08fcd8f4fbaf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Average Age: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Supported join operations in Spark SQL. Broadcast joins. Basic principles of functioning. Which types of broadcast-based joins exist and what are the differences in their functioning? Conditions of when one can apply these types of joins.\n",
        "\n",
        "- Inner Join. Returns rows that have matching values in both tables\n",
        "\n",
        "- Left Outer Join (Left Join) - returns all rows from the left table, and the matched rows from the right table.\n",
        "\n",
        "- Right Join - from the right table, and matched left values\n",
        "\n",
        "- Full Join -  Returns all rows when there is a match in one of the tables. Rows without matches in the other table will have NULL values.\n",
        "\n",
        "- Cross Join (Cartesian Product): Returns the Cartesian product of both tables."
      ],
      "metadata": {
        "id": "6fKvyKA06khS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcast Joins are a type of join where one of the smaller DataFrame is broadcast to all worker nodes. This can significantly improve the performance of joins involving a large and a small DataFrame by avoiding the need for data shuffling.\n",
        "\n",
        "Basic Principles of Functioning:\n",
        "1. Broadcasting\n",
        "2. Joining\n",
        "3. Efficiency\n",
        "\n",
        "Types of Joins:\n",
        "1. Broadcast Hash Join (BHJ)\n",
        "2. Broadcast Loop join\n",
        "\n",
        "\n",
        "Conditions for Applying Broadcast joins:\n",
        "1. Size of DataFrame - dataframe should be small enough to fit into the memory of each worker node\n",
        "2. Spark Configuration - spark.sql.autoBroadcastJoinThreshold\n",
        "3. Join Conditions: equi- joins = joins with equal conditions"
      ],
      "metadata": {
        "id": "CFAQXCHADiVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast Join\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "spark = SparkSession.builder.appName('BroadcastJoinExample').getOrCreate()\n",
        "\n",
        "df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], [\"id\", \"value1\"])\n",
        "df2 = spark.createDataFrame([(1, 'X'), (2, 'Y')], [\"id\", \"value2\"])\n",
        "\n",
        "df1.join(broadcast(df2), df1.id == df2.id).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymLkae7CZ7T-",
        "outputId": "b357e996-35e0-4532-a08f-2d770aaa0d97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+------+\n",
            "| id|value1| id|value2|\n",
            "+---+------+---+------+\n",
            "|  1|     A|  1|     X|\n",
            "|  2|     B|  2|     Y|\n",
            "+---+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JoinExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create DataFrames\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 1000),\n",
        "    (2, \"Bob\", 1500),\n",
        "    (3, \"Charlie\", 1200),\n",
        "    (4, \"David\", 1800)\n",
        "], [\"emp_id\", \"emp_name\", \"salary\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (1, \"HR\"),\n",
        "    (2, \"Engineering\"),\n",
        "    (3, \"Finance\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "# Show DataFrames\n",
        "print(\"Employees DataFrame:\")\n",
        "employees.show()\n",
        "\n",
        "print(\"Departments DataFrame:\")\n",
        "departments.show()\n",
        "\n",
        "# Perform Join Operations\n",
        "\n",
        "# 1. Inner Join\n",
        "inner_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"inner\")\n",
        "print(\"Inner Join Result:\")\n",
        "inner_join_df.show()\n",
        "\n",
        "# 2. Left Outer Join (Left Join)\n",
        "left_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"left_outer\")\n",
        "print(\"Left Outer Join Result:\")\n",
        "left_join_df.show()\n",
        "\n",
        "# 3. Right Outer Join (Right Join)\n",
        "right_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"right_outer\")\n",
        "print(\"Right Outer Join Result:\")\n",
        "right_join_df.show()\n",
        "\n",
        "# 4. Full Outer Join (Full Join)\n",
        "full_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"outer\")\n",
        "print(\"Full Outer Join Result:\")\n",
        "full_join_df.show()\n",
        "\n",
        "# 5. Cross Join (Cartesian Product)\n",
        "cross_join_df = employees.crossJoin(departments)\n",
        "print(\"Cross Join Result:\")\n",
        "cross_join_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WF6W_5Cajlh",
        "outputId": "576ff296-3e56-433d-b00a-bea6e0a38bf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees DataFrame:\n",
            "+------+--------+------+\n",
            "|emp_id|emp_name|salary|\n",
            "+------+--------+------+\n",
            "|     1|   Alice|  1000|\n",
            "|     2|     Bob|  1500|\n",
            "|     3| Charlie|  1200|\n",
            "|     4|   David|  1800|\n",
            "+------+--------+------+\n",
            "\n",
            "Departments DataFrame:\n",
            "+-------+-----------+\n",
            "|dept_id|  dept_name|\n",
            "+-------+-----------+\n",
            "|      1|         HR|\n",
            "|      2|Engineering|\n",
            "|      3|    Finance|\n",
            "+-------+-----------+\n",
            "\n",
            "Inner Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Left Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|   NULL|       NULL|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Right Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Full Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|   NULL|       NULL|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Cross Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      1|         HR|\n",
            "|     1|   Alice|  1000|      2|Engineering|\n",
            "|     1|   Alice|  1000|      3|    Finance|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     2|     Bob|  1500|      3|    Finance|\n",
            "|     3| Charlie|  1200|      1|         HR|\n",
            "|     4|   David|  1800|      1|         HR|\n",
            "|     3| Charlie|  1200|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|      2|Engineering|\n",
            "|     4|   David|  1800|      3|    Finance|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Sort Merge Join. What is it for? Basic principles of functioning. Conditions of when one can apply these types of joins. Advantages and disadvantages in comparison with other joins."
      ],
      "metadata": {
        "id": "T6nPLQBC6nAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Bucketing: what it is, its advantages and basic principles of functioning.\n",
        "\n",
        "\n",
        "Bucketing is a technique used in Apache Spark to optimize data storage and query performance by organizing data into a specified number of buckets based on the hash value of a column's value.\n",
        "\n",
        "Advantages:\n",
        "- Query Performance\n",
        "- Optimized Storage\n",
        "- Controlled Data Distribution"
      ],
      "metadata": {
        "id": "nu56XFDb6o68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BucketingExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29), (\"David\", 42), (\"Eve\", 36)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Bucketing Configuration\n",
        "num_buckets = 4\n",
        "bucket_column = \"Age\"\n",
        "df_bucketed = df.repartitionByRange(num_buckets, bucket_column)\n",
        "\n",
        "# Show the bucketed DataFrame\n",
        "df_bucketed.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twkXpXIMckLL",
        "outputId": "a28d312a-ccee-4eb3-f0c5-b3734c50ef57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|Catherine| 29|\n",
            "|      Eve| 36|\n",
            "|    David| 42|\n",
            "|      Bob| 45|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClickHouse\n",
        "\n",
        "### 1.What is ClickHouse? Which components does its architecture consist of?\n",
        "\n",
        "Clickhouse is developed by yandex.\n",
        "\n",
        "- It is used for the online analytical processing (OLAP) and real- time analytics on large volumes of data\n",
        "\n",
        "Key features:\n",
        "- High performance\n",
        "- Columnar storage. Data is stored in columns, not rows and it makes faster\n",
        "\n",
        "Architecture:\n",
        "1. Storage Layer:\n",
        "\n",
        "- MergeTree Engines - a family of storage engines in Clickhouse\n",
        "\n",
        "- Data parts\n",
        "- Columnar Storage\n",
        "\n",
        "2. Query Processing Layer\n",
        "\n",
        "3. Replication and Distribution\n",
        "\n",
        "4. Data Ingestion and Export\n",
        "\n",
        "5. MetaData and Configuration: ZooKeeper\n",
        "\n",
        "6. Client Interfaces - Native Clickhouse Client, Https Interface."
      ],
      "metadata": {
        "id": "g01UoBuc2WbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. What is Materialized View and what is its purpose? How does Materialized Views work? Does it make sense to create a materialized view over a distributed table? If the answer is positive, why?\n",
        "\n",
        "- Materialized Views in ClickHouse is a database object that contains the results of a query.\n",
        "\n",
        "Purpose of Materialized Views:\n",
        "- Performance Optimization\n",
        "- Efficient Aggregation\n",
        "\n",
        "How it works?\n",
        "- Creation: \"Create Materialized View\"\n",
        "- Data Ingestion: as new data is inserted into the source table, the materialized view is automatically updated.\n",
        "\n",
        "- Storage: The results of the query are stored on disk"
      ],
      "metadata": {
        "id": "mMhLtUqFB_zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Materialized view over Distributed Tables:\n",
        "\n",
        "Yes, it makes sense, because:\n",
        "- Precomputed results: Aggregations and complex computations can be precomputed, reducing the load on distributed query execution\n",
        "\n",
        "- Performance: avoiding repeated calculations\n",
        "\n",
        "- Consistency: Maintaining data consistency by updated sync with the dist. source table"
      ],
      "metadata": {
        "id": "F6SRkPOxGE1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Describe MergeTree engine work principles. How does ClickHouse store data in the filesystem? How does parts merging works from in terms of files?\n",
        "\n",
        "- MergeTree Engine: A high-performance storage engine in ClickHouse, supporting efficient data ingestion, storage, and querying with columnar storage and primary key indexing.\n",
        "\n",
        "Work Principles of Merge Tree Engine:\n",
        "1. Data Ingestion- Uploading data\n",
        "2. Primary Key and Indexing -\n",
        "3. Storage of Data in Filesystem\n",
        "\n",
        "How parts Mergin Works:\n",
        "1. Background Merging: Clickhouse continuously merges smaller parts into larger parts in the background, a process known as compaction\n",
        "\n",
        "\n",
        "2. Merge Algorithm\n",
        "3. Files Involved in Merging"
      ],
      "metadata": {
        "id": "EgXEVVI_CE8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is sharding, why is it useful and how is it implemented in ClickHouse? How can one distribute data into shards using ClickHouse?\n",
        "\n",
        "Sharding is a database architecture pattern that involves partitioning data across multiple servers or nodes to distribute the load and improve performance. Each partition is called a shard. In the context of ClickHouse, sharding helps manage large datasets efficiently by distributing them across multiple nodes in a cluster."
      ],
      "metadata": {
        "id": "uHL1YrQXCHTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. How does AggregatingMergeTree work? Are there any differences between aggregation functions in AggregatingMergeTree and simple queries?\n"
      ],
      "metadata": {
        "id": "2hpThfAOCI8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6. Why is it recommended inserting data into ClickHouse by large batches? What can one do if data arrives in short portions frequently? Name and explain existing mechanisms in ClickHouse to deal with the problem.\n"
      ],
      "metadata": {
        "id": "-p2F_uXSCKph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Describe sparse index work principles. How is it implemented in ClickHouse? How can we set up indexes in ClickHouse?"
      ],
      "metadata": {
        "id": "uPwMAiSLCMJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kubernetes\n",
        "\n",
        "### 1. What is Kubernetes? Its architecture, main components.\n",
        "\n",
        "Kubernetes is an open-source container orchestration platform designed to automate the deployment, scaling, and operation of application containers.\n",
        "\n",
        "Main components of Kubernetes:\n",
        "1. Master Components:\n",
        "- API server\n",
        "- etcd. Key- value store used for all cluster data storage\n",
        "- Controller Manager\n",
        "- Scheduler\n",
        "\n",
        "2. Worker Node Components:\n",
        "- Kubelet\n",
        "- Kube- Proxy\n",
        "- Container Runtime"
      ],
      "metadata": {
        "id": "1D9jUmwq2niK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. What is Pod? Difference between pod and container.\n",
        "\n",
        "Pod - smallest and simplest unit of deployment. It represents the group of one or more containers that share storage (volumes), network, and lifecycle settings.\n",
        "\n",
        "Difference between container and pod:\n",
        "Pods:\n",
        "- Scope: A Pod can contain multiple containers that are tightly coupled and share resources. Containers, on the other hand, are standalone units that encapsulate a single process or service.\n",
        "- Networking: Containers within a Pod share the same network namespace and can communicate via localhost. Containers in different Pods require network communication through Kubernetes networking.\n",
        "- Management: Pods are managed by Kubernetes and can be created, scaled, and deleted as a unit. Containers are managed within Pods and are not directly orchestrated by Kubernetes outside of the Pod context.\n",
        "\n",
        "\n",
        "A container is a standard unit of software that packages up code and all its dependencies, so the application runs quickly and reliably from one computing environment to another.\n"
      ],
      "metadata": {
        "id": "SfHLTe6sUrn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is Service? How does it work? Service types: nodeport, clusterip, headless service.\n",
        "\n",
        "In Kubernetes, a Service is an abstraction that defines a logical set of Pods and a policy by which to access them. It provides a stable endpoint (IP address and port) for connecting to a group of Pods, regardless of their individual IP addresses or the nodes they run on. Services enable communication between various components in a Kubernetes cluster and external clients.\n",
        "\n",
        "Service Types:\n",
        "- NodePort - Exposes the Service on each Node's IP address at a static port. It creates a high-port (30000-32767) on each Node that redirects to the Service.\n",
        "- ClusterIp - This IP address is accessible only within the Kubernetes cluster. The default internal port.\n",
        "- LoadBalancer. External IP address thhat routes traffic to the Service\n",
        "- Headless service - The ClusterIP is set to None and no virtual IP is allocated. Useful for stateful applications where each Pod neds a unique identity"
      ],
      "metadata": {
        "id": "j9wMghgWUvZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Container resources: how is it implemented? CPU, RAM, Storage. QoS classes.\n",
        "\n",
        "1. CPU and RAM: Managed using requests and limits to ensure predictable scheduling and prevent resource starvation.\n",
        "2. Storage: Managed through Persistent Volumes and Persistent Volume Claims, allowing persistent storage for applications.\n",
        "3.  QOS (Quality of Service) Classes - Assigns Pods to Guaranteed, Burstable, or BestEffort classes based on their resource requests and usage patterns, ensuring fair resource allocation in the cluster.\n",
        "- Guaranteed: Pods have both CPU and memory requests set and these requests equal their limits\n",
        "\n",
        "- Burstable: The usage of CPU and RAM exceeds the limits and requests\n",
        "\n",
        "- BestEffort: Pods doesnt have the requests set and they receive lowest priority after other pods.\n"
      ],
      "metadata": {
        "id": "Byq2RsfMUw_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. PV & PVC: what is it and how does it work? PV types. Volume provisioner.\n",
        "\n",
        "Persistent Volume (PV) - A storage resource in a Kubernetes Cluster. They are resources in the cluster just like nodes and cluster resources.\n",
        "\n",
        "Persistent Volume Claim (PVC) - A user's request for storage resources that binds to a PV.\n",
        "\n",
        "PV types:\n",
        "- Manual: Created by adminstrators\n",
        "- Dynamic: Automatically and dynamically provisioned based on StorageClasses.\n",
        "\n",
        "Volume Provisioner: A component that automates the creation of PVs based on user-defined StorageClasses.\n",
        "\n",
        "Common Provisioners\n",
        "- kubernetes.io/aws-ebs: AWS Elastic Block Store\n",
        "- kubernetes.io/gce-pd: Google Compute Engine Persistent Disk\n",
        "- kubernetes.io/azure-disk: Azure Disk\n",
        "- kubernetes.io/nfs: Network File System\n",
        "- kubernetes.io/host-path: Local storage on the host"
      ],
      "metadata": {
        "id": "gTrS26OmUy1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. StatefulSet. Difference from Deployment/Pod.\n",
        "\n",
        "In Kubernetes, a StatefulSet is a type of resource that is used to manage stateful applications. Stateful applications are those that require each instance (or replica) to have a unique identity and stable, persistent storage. Examples of stateful applications include databases and distributed systems. Such as Kafka, Elastic Search\n",
        "\n",
        "- Deployment: Best for Stateless applications and pods are interchangeable and do not need stable network identities or persistent storage.\n",
        "\n",
        "- Statefulset is need for unique identifiers"
      ],
      "metadata": {
        "id": "KLgcyOfBU0P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 7. Label selector. What is it? How is it used?\n",
        "\n",
        "Label selector are a core consept in Kubernetes used to organize and select subsets of objects on their labels.\n",
        "\n",
        "Labels are key- value pairs attached to Kubernetes objects, such as Pods, Nodes, Services.\n",
        "\n",
        "Types:\n",
        "1. Equality- Based Selectors - selects exact match key- value\n",
        "2. Set- Based Selectors - match labels based on set operations\n",
        "\n",
        "Usage: Widely used in various Kubernetes resources like Services, ReplicaSets, Jobs, Network Policies, and more, to dynamically group and manage objects efficiently.\n",
        "\n",
        "By using label selectors, We can dynamically manage and control Kubernetes resources, making it easier to scale, organize, and maintain your applications.\n"
      ],
      "metadata": {
        "id": "mL1g--4WU1pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7iHEtxiWb9q"
      },
      "outputs": [],
      "source": []
    }
  ]
}